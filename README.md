# Continual Learning with Attention-Driven Feature Adaptation and Memory-Based Analytic Classifier

This repository contains the **research implementation of FAAD-Mem**  
(Feature-Adapted Attention-Driven Analytic Learning with Memory Augmentation and Knowledge Distillation).  

FAAD-Mem is designed for **class-incremental continual learning (CIL)** and is currently under review.  

---

## ğŸ” Overview
Continual learning aims to acquire new tasks without forgetting previously learned knowledge.  
The main challenge is **catastrophic forgetting**, where new training overwrites old knowledge.  

FAAD-Mem addresses this problem through:
- **Frozen CNN backbone** for stable feature extraction  
- **Feature adaptation + attention module** for re-alignment  
- **Recursive analytic classifier** solved in closed form  
- **Prototype-based memory bank** (exemplar-free replay)  
- **EMA teacher distillation** with cosine-annealed scheduling  
- **In-loop bias correction** to mitigate prediction drift  

---

## ğŸ“‚ Repository Structure

â”œâ”€â”€ config.py # Experiment arguments
â”œâ”€â”€ main.py # Training script (base, re-align, incremental)
â”œâ”€â”€ FAADMem.py # FAAD-Mem core model
â”œâ”€â”€ datasets/ # Dataset loaders
â”œâ”€â”€ results/ # Output logs (IL.csv, base_training.csv)
â””â”€â”€ README.md # Documentation


---

## âš™ï¸ Installation

We recommend **Python 3.8+** and **PyTorch â‰¥ 1.11**.

```bash
git clone https://github.com/yourusername/FAAD-Mem.git
cd FAAD-Mem
conda create -n faadmem python=3.8
conda activate faadmem
pip install -r requirements.txt

